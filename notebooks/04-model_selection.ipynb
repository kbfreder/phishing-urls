{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kendra/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:886: MatplotlibDeprecationWarning: \n",
      "examples.directory is deprecated; in the future, examples will be found relative to the 'datapath' directory.\n",
      "  \"found relative to the 'datapath' directory.\".format(key))\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import feather\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# project_dir = os.path.dirname(os.path.dirname(os.path.abspath(os.path.curdir)))\n",
    "project_dir = os.path.dirname(os.path.abspath(os.path.curdir))\n",
    "new_path = os.path.join(project_dir, 'src')\n",
    "sys.path.append(new_path)\n",
    "\n",
    "import util as u\n",
    "from model import pipeline as p\n",
    "\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = feather.read_dataframe('../data/processed/train_df.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7449443, 27)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "benign      0.998858\n",
       "phishing    0.001142\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df['label'].value_counts(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take subset of data for initial model assessment\n",
    "df = full_df.sample(frac=0.2).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1489889, 27)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "benign      0.998892\n",
       "phishing    0.001108\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure we've retained proportion of labels\n",
    "df['label'].value_counts(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert int to float64. Avoids a warning error while fitting some algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_cols = df.select_dtypes(include='int').columns\n",
    "int_cols = [col for col in int_cols if re.search('_ind', col) is None]\n",
    "\n",
    "for col in int_cols:\n",
    "    df[col] = df[col].astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From EDA: Correlated features:\n",
    "- length_url with length_path, url_X_cnt, and url_entropy\n",
    "    - convert these to fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_convert = ['length_path', 'length_domain', 'url_slash_cnt',\n",
    "       'url_digit_cnt', 'url_special_char_cnt', 'url_reserved_char_cnt']\n",
    "\n",
    "for col in cols_to_convert:\n",
    "    new_col_name = col + '_frac_url_len'\n",
    "    df[new_col_name] = df[col] / df['length_url'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't need this, since can't include suffix, since benign only contains .com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_base_suffix(s):\n",
    "#     if len(s) > 0:\n",
    "#         return s.split('.')[0]\n",
    "#     else:\n",
    "#         return s\n",
    "\n",
    "# df['base_suffix'] = df['suffix'].apply(extract_base_suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into X, y and a further train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.reset_index(inplace=True, drop=True)\n",
    "y.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'label'\n",
    "X = df.drop(columns=target)\n",
    "y = df[target]\n",
    "\n",
    "# some algorithms require encoding of our target / label\n",
    "enc = LabelEncoder()\n",
    "y_enc = enc.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_enc, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = df.columns\n",
    "\n",
    "target = 'label'\n",
    "\n",
    "proc_dict = {\n",
    "#     'base_suffix':[p.Consolidate(1), OneHotEncoder(handle_unknown='ignore')]\n",
    "            }\n",
    "\n",
    "num_cols = [col for col in all_cols if re.search('_cnt', col) is not None] + \\\n",
    "            ['length_url', 'hostname_entropy', 'url_entropy']\n",
    "\n",
    "bool_cols = [col for col in all_cols if re.search('_ind', col) is not None]\n",
    "\n",
    "pass_thru_cols = [col for col in all_cols if re.search('_frac_url_len', col) is not None]\n",
    "\n",
    "for col in num_cols:\n",
    "    proc_dict[col] = [StandardScaler()]\n",
    "\n",
    "for col in bool_cols + pass_thru_cols:\n",
    "    proc_dict[col] = [p.PassThrough()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, do not include suffix, as benign only contains '.com', so this is a leaky variable of sorts. Also exclude features that were found to have low association / correlation with the target (i.e. HEX indicator, 'abuse' suspicious word indicator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cols = ['subdomain_null_ind', 'subdomain_www_ind', 'length_url',\n",
    "        'domain_dot_cnt', 'path_dot_cnt',\n",
    "       'hostname_dash_cnt',\n",
    "       'hostname_entropy', 'url_entropy', 'php_ind', 'admin_ind',\n",
    "       'verification_ind', \n",
    "              'length_path_frac_url_len',\n",
    "       'length_domain_frac_url_len', \n",
    "       'url_slash_cnt_frac_url_len',\n",
    "       'url_digit_cnt_frac_url_len', 'url_special_char_cnt_frac_url_len',\n",
    "       'url_reserved_char_cnt_frac_url_len']\n",
    "\n",
    "preproc_pipe = FeatureUnion(p.gen_pipeline(model_cols, proc_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `assess_model` function in the custom `util` (`u`) module peforms 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with good old Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'util' from '/Users/kendra/Documents/data_science/Projects/phishing-urls/src/util.py'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kendra/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/kendra/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/kendra/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/kendra/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/kendra/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(penalty='l2', random_state=19, solver='lbfgs')\n",
    "lr_res = u.assess_model_df(preproc_pipe, model, X, y_enc, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Precision-0                    0.998913\n",
       "Recall-0 (Specificty)          0.999997\n",
       "F1score-0                      0.999455\n",
       "Precision-1                    0.887778\n",
       "Recall-1 (Sensitivity)         0.019379\n",
       "F1score-1                      0.037857\n",
       "TN                        297646.600000\n",
       "FN                           323.800000\n",
       "FP                             1.000000\n",
       "TP                             6.400000\n",
       "AUC                            0.826314\n",
       "Accuracy                       0.998910\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Failure to converge could be an indication of a \"leaky\" variable, or a feature that is \"too good\" at predicting the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "nb_res = u.assess_model_df(preproc_pipe, model, X, y_enc, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Precision-0                    0.998989\n",
       "Recall-0 (Specificty)          0.996595\n",
       "F1score-0                      0.997791\n",
       "Precision-1                    0.028809\n",
       "Recall-1 (Sensitivity)         0.090840\n",
       "F1score-1                      0.043734\n",
       "TN                        296634.200000\n",
       "FN                           300.200000\n",
       "FP                          1013.400000\n",
       "TP                            30.000000\n",
       "AUC                            0.832448\n",
       "Accuracy                       0.995592\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As RandomForest takes longer to fit, let's start with one round:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 602.4431281089783 seconds\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "start_time = time.time()\n",
    "rf_res = u.assess_model_no_cv(preproc_pipe, model, X_train, y_train, X_test, y_test)\n",
    "duration = time.time() - start_time\n",
    "print('Elapsed time: {} seconds'.format(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Precision-0                    0.999125\n",
       "Recall-0 (Specificty)          0.999984\n",
       "F1score-0                      0.999554\n",
       "Precision-1                    0.916667\n",
       "Recall-1 (Sensitivity)         0.168367\n",
       "F1score-1                      0.284483\n",
       "TN                        372075.000000\n",
       "FN                           326.000000\n",
       "FP                             6.000000\n",
       "TP                            66.000000\n",
       "AUC                            0.846035\n",
       "Accuracy                       0.999109\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few notes on why I didn't choose other models to examine:\n",
    "- Logistic Regression using L1 penalty didn't converge either (not shown)\n",
    "- SVMs are not efficient on large datasets (est O(n^3))\n",
    "- KNNs also are not efficient on large datasets (O(nd))\n",
    "- I would have liked to examine an implementation of Gradient Boosting of Decision Trees, but I did not, given time contrainsts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = {'Log Reg': lr_res,\n",
    "            'Naive Bayes': nb_res,\n",
    "            'Random Forest': rf_res}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Log Reg</th>\n",
       "      <th>Naive Bayes</th>\n",
       "      <th>Random Forest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Precision-0</th>\n",
       "      <td>0.998913</td>\n",
       "      <td>0.998989</td>\n",
       "      <td>0.999125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall-0 (Specificty)</th>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.996595</td>\n",
       "      <td>0.999984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1score-0</th>\n",
       "      <td>0.999455</td>\n",
       "      <td>0.997791</td>\n",
       "      <td>0.999554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision-1</th>\n",
       "      <td>0.887778</td>\n",
       "      <td>0.028809</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall-1 (Sensitivity)</th>\n",
       "      <td>0.019379</td>\n",
       "      <td>0.090840</td>\n",
       "      <td>0.168367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1score-1</th>\n",
       "      <td>0.037857</td>\n",
       "      <td>0.043734</td>\n",
       "      <td>0.284483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TN</th>\n",
       "      <td>297646.600000</td>\n",
       "      <td>296634.200000</td>\n",
       "      <td>372075.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FN</th>\n",
       "      <td>323.800000</td>\n",
       "      <td>300.200000</td>\n",
       "      <td>326.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FP</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1013.400000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TP</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>66.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC</th>\n",
       "      <td>0.826314</td>\n",
       "      <td>0.832448</td>\n",
       "      <td>0.846035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.998910</td>\n",
       "      <td>0.995592</td>\n",
       "      <td>0.999109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Log Reg    Naive Bayes  Random Forest\n",
       "Precision-0                  0.998913       0.998989       0.999125\n",
       "Recall-0 (Specificty)        0.999997       0.996595       0.999984\n",
       "F1score-0                    0.999455       0.997791       0.999554\n",
       "Precision-1                  0.887778       0.028809       0.916667\n",
       "Recall-1 (Sensitivity)       0.019379       0.090840       0.168367\n",
       "F1score-1                    0.037857       0.043734       0.284483\n",
       "TN                      297646.600000  296634.200000  372075.000000\n",
       "FN                         323.800000     300.200000     326.000000\n",
       "FP                           1.000000    1013.400000       6.000000\n",
       "TP                           6.400000      30.000000      66.000000\n",
       "AUC                          0.826314       0.832448       0.846035\n",
       "Accuracy                     0.998910       0.995592       0.999109"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df = pd.DataFrame(res_dict)\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fpr(tn, fp):\n",
    "    return 1 - (tn / (tn + fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.359677685943474e-06"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr(297646.600000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0034046973669534797"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr(296634.200000, 1013.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6125521055898595e-05"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr(372075.000000, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest performs the best (per F1-score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the algorithm took X min to fit-predict one iteration, 3- to 5-fold cross validation inside a `GridSearchCV` would be time prohibitive. Continue to use our train-test-split of our subset of our train data (so, we are still sufficiently protected from out hold-out test set), and perform tuning \"by hand\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0665188470066519"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10, criterion='gini')\n",
    "pipe = make_pipeline(preproc_pipe, rf)\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred_test = pipe.predict(X_test)\n",
    "f1_score(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12700729927007298"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_train = pipe.predict(X_train)\n",
    "f1_score(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
